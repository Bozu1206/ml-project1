{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys \n",
    "import warnings\n",
    "import costs\n",
    "import gradient_descent\n",
    "import stochastic_gradient_descent\n",
    "import implementations\n",
    "import metrics\n",
    "\n",
    "sys.path.append(\"/home/souly/Desktop/ml/ml-project1\")\n",
    "from src import json_parser, helpers, preprocessing\n",
    "from src.helpers import split_data_rand\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_csv(\"../data/raw/x_train.csv\")\n",
    "y_df = pd.read_csv(\"../data/raw/y_train.csv\")\n",
    "\n",
    "features = json_parser.parse_json_file(\"../data/raw/features.json\")\n",
    "\n",
    "def balance(y, x):\n",
    "    positive_indices = np.where(y == 1)[0]\n",
    "    negative_indices = np.where(y != 1)[0]\n",
    "\n",
    "    min_samples = min(len(positive_indices), len(negative_indices))\n",
    "    downsampled_negative_indices = random.sample(list(negative_indices), min_samples)\n",
    "    balanced_indices = np.concatenate([positive_indices, downsampled_negative_indices])\n",
    "\n",
    "    y = y[balanced_indices]\n",
    "    x = x[balanced_indices]\n",
    "\n",
    "    return y, x\n",
    "\n",
    "def k_folds(y, x, k):\n",
    "    fold_size = len(y) // k\n",
    "    x_folds = []\n",
    "    y_folds = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i+1) * fold_size if i < k + 1 else None\n",
    "        y_fold = y[start:end]\n",
    "        x_fold = x[start:end]\n",
    "        y_folds.append(y_fold)\n",
    "        x_folds.append(x_fold)\n",
    "\n",
    "    return y_folds, x_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Model(ABC):\n",
    "    def __init__(self, penalty, learning_rate, max_iter, initializer=\"none\"):\n",
    "        self.penalty = penalty\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.initializer = initializer\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = self.__class__.__name__ + \"(\"\n",
    "        for attr, value in self.__dict__.items():\n",
    "            if attr != \"w\":\n",
    "                s += attr + \"=\" + str(value) + \", \"\n",
    "        s = s[:-2] + \")\"\n",
    "        return s\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    def fit(self, x, y):\n",
    "        if self.initializer == \"he\":\n",
    "            self.w = np.random.normal(0.0, 2 / x.shape[1], x.shape[1])\n",
    "        elif self.initializer == \"godot\":\n",
    "            self.w = np.random.normal(0.0, 2 / (x.shape[1] + 2), x.shape[1])\n",
    "        else:\n",
    "            self.w = np.ones(x.shape[1])\n",
    "        self.w, loss = implementations.reg_logistic_regression(y, x, self.penalty, self.w, self.max_iter, self.learning_rate)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        preds = costs.sigmoid(x.dot(self.w))\n",
    "        preds = (preds >= 0.5).astype(int)\n",
    "        return preds\n",
    "\n",
    "    def score(self, x, y):\n",
    "        return metrics.compute_accuracy(y, self.predict(x))\n",
    "\n",
    "class LinearRegression(Model):\n",
    "    def fit(self, x, y):\n",
    "        self.w = np.ones(x.shape[1]) # TODO: try different initializer\n",
    "        return implementations.mean_squared_error_sgd(y, x, self.penalty, self.w, self.max_iter, self.learning_rate)\n",
    "\n",
    "    def predict(self, x):\n",
    "        preds = self.w.T@x\n",
    "        preds = (preds >= 0.5).as_type(int)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.001, learning_rate=0.5, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.738, std=0.041\n",
      "| F1 SCORE: mean=0.357, std=0.016\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0001, learning_rate=0.5, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.743, std=0.04\n",
      "| F1 SCORE: mean=0.359, std=0.017\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=1e-05, learning_rate=0.5, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.766, std=0.027\n",
      "| F1 SCORE: mean=0.369, std=0.023\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0, learning_rate=0.5, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.765, std=0.028\n",
      "| F1 SCORE: mean=0.37, std=0.024\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.001, learning_rate=0.1, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.755, std=0.006\n",
      "| F1 SCORE: mean=0.365, std=0.013\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0001, learning_rate=0.1, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.756, std=0.005\n",
      "| F1 SCORE: mean=0.366, std=0.015\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=1e-05, learning_rate=0.1, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.755, std=0.006\n",
      "| F1 SCORE: mean=0.364, std=0.013\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0, learning_rate=0.1, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.757, std=0.006\n",
      "| F1 SCORE: mean=0.366, std=0.016\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.001, learning_rate=0.05, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.755, std=0.006\n",
      "| F1 SCORE: mean=0.364, std=0.014\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0001, learning_rate=0.05, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.755, std=0.004\n",
      "| F1 SCORE: mean=0.364, std=0.014\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=1e-05, learning_rate=0.05, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.757, std=0.007\n",
      "| F1 SCORE: mean=0.366, std=0.014\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0, learning_rate=0.05, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.756, std=0.007\n",
      "| F1 SCORE: mean=0.365, std=0.014\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.001, learning_rate=0.01, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.753, std=0.008\n",
      "| F1 SCORE: mean=0.36, std=0.013\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0001, learning_rate=0.01, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.753, std=0.006\n",
      "| F1 SCORE: mean=0.36, std=0.013\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=1e-05, learning_rate=0.01, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.751, std=0.007\n",
      "| F1 SCORE: mean=0.358, std=0.014\n",
      "===========================================================\n",
      "10-folds: LogisticRegression(penalty=0.0, learning_rate=0.01, max_iter=10000, initializer=none)\n",
      "| ACCURACY: mean=0.75, std=0.005\n",
      "| F1 SCORE: mean=0.356, std=0.013\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "seed = 42\n",
    "degree = 1\n",
    "split_ratio = 0.8\n",
    "k = 10\n",
    "max_iter = 10000\n",
    "learning_rate = 0.5\n",
    "\n",
    "x = x_df.values\n",
    "y = y_df[\"_MICHD\"].values\n",
    "y[y == -1] = 0\n",
    "\n",
    "x = preprocessing.clean_data(features, x, median_estimator=False, do_poly=False, do_one_hot=True)\n",
    "\n",
    "y_folds, x_folds = k_folds(y, x, k)\n",
    "\n",
    "# grid search\n",
    "for initializer in [\"none\", \"he\", \"godot\"]:\n",
    "    for penalty in [0.001, 0.0001, 0.00001]:\n",
    "        print(\"===========================================================\")\n",
    "        train_losses = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        model = LogisticRegression(penalty, learning_rate, max_iter, initializer)\n",
    "        print(str(k) + \"-folds: \" + str(model))\n",
    "        it = 1\n",
    "        # k-folds\n",
    "        for y_f, x_f in zip(y_folds, x_folds):\n",
    "            split_index = int(len(x_f)*split_ratio)\n",
    "            x_train = x_f[:split_index]\n",
    "            y_train = y_f[:split_index]\n",
    "            x_test = x_f[split_index:]\n",
    "            y_test = y_f[split_index:]\n",
    "            y_train, x_train = balance(y_train, x_train)\n",
    "            train_loss = model.fit(x_train, y_train)\n",
    "            acc = model.score(x_test, y_test)\n",
    "            pred = model.predict(x_test)\n",
    "            score = f1_score(y_test, pred)\n",
    "            train_losses.append(train_loss)\n",
    "            accuracies.append(acc)\n",
    "            f1_scores.append(score)\n",
    "            # print(\"| [\" + str(it) + \"/\" + str(k) + \"] folds: acc=\" + str(round(acc, 3)) \n",
    "            #     + \", f1_score=\" + str(round(score, 3)) \n",
    "            #     + \", train_loss=\" + str(round(train_loss, 3)))\n",
    "            # it += 1\n",
    "        accuracies = np.array(accuracies)\n",
    "        f1_scores = np.array(f1_scores)\n",
    "        print(\"| ACCURACY: mean=\" + str(round(np.mean(accuracies), 3)) + \", std=\" + str(round(np.std(accuracies), 3)))\n",
    "        print(\"| F1 SCORE: mean=\" + str(round(np.mean(f1_scores), 3)) + \", std=\" + str(round(np.std(f1_scores), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
