{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys \n",
    "import warnings\n",
    "\n",
    "sys.path.append(\"/home/souly/Desktop/ml/ml-project1\")\n",
    "from src import json_parser, helpers, preprocessing\n",
    "from src.helpers import split_data_rand\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from src.preprocessing import undefined_to_median, undefined_to_avg, prune_undefined\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_csv(\"../data/raw/x_train.csv\")\n",
    "y_df = pd.read_csv(\"../data/raw/y_train.csv\")\n",
    "\n",
    "features = json_parser.parse_json_file(\"../data/raw/features.json\")\n",
    "\n",
    "def balance(y, x):\n",
    "    positive_indices = np.where(y == 1)[0]\n",
    "    negative_indices = np.where(y != 1)[0]\n",
    "\n",
    "    min_samples = min(len(positive_indices), len(negative_indices))\n",
    "    downsampled_negative_indices = random.sample(list(negative_indices), min_samples)\n",
    "    balanced_indices = np.concatenate([positive_indices, downsampled_negative_indices])\n",
    "\n",
    "    y = y[balanced_indices]\n",
    "    x = x[balanced_indices]\n",
    "\n",
    "    return y, x\n",
    "\n",
    "def k_folds(y, x, k):\n",
    "    fold_size = len(y) // k\n",
    "    x_folds = []\n",
    "    y_folds = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i+1) * fold_size if i < k + 1 else None\n",
    "        y_fold = y[start:end]\n",
    "        x_fold = x[start:end]\n",
    "        y_folds.append(y_fold)\n",
    "        x_folds.append(x_fold)\n",
    "\n",
    "    return y_folds, x_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K-folds on LogisticRegression\n",
      "K=10\n",
      "\tACCURACY: mean=0.7571689776017065, std=0.006546582125741302\n",
      "\tF1 SCORE: mean=0.36596539468310724, std=0.015320453879347603\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "seed = 42\n",
    "degree = 1\n",
    "split_ratio = 0.8\n",
    "k = 10\n",
    "\n",
    "x = x_df.values\n",
    "y = y_df[\"_MICHD\"].values\n",
    "\n",
    "x = preprocessing.clean_data(features, x, median_estimator=True, do_poly=False, do_one_hot=True)\n",
    "\n",
    "y_folds, x_folds = k_folds(y, x, k)\n",
    "\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "print(\"Running K-folds on LogisticRegression\")\n",
    "print(\"K=\" + str(k))\n",
    "\n",
    "it = 1\n",
    "\n",
    "for y_f, x_f in zip(y_folds, x_folds):\n",
    "    split_index = int(len(x_f)*split_ratio)\n",
    "    \n",
    "    x_train = x_f[:split_index]\n",
    "    y_train = y_f[:split_index]\n",
    "    x_test = x_f[split_index:]\n",
    "    y_test = y_f[split_index:]\n",
    "    \n",
    "    y_train, x_train = balance(y_train, x_train)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = LogisticRegression(penalty=\"l2\")\n",
    "        model.fit(x_train, y_train)\n",
    "        acc = model.score(x_test, y_test)\n",
    "        accuracies.append(acc)\n",
    "        pred = model.predict(x_test)\n",
    "        score = f1_score(y_test, pred)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "accuracies = np.array(accuracies)\n",
    "f1_scores = np.array(f1_scores)\n",
    "print(\"\\tACCURACY: mean=\" + str(np.mean(accuracies)) + \", std=\" + str(np.std(accuracies)))\n",
    "print(\"\\tF1 SCORE: mean=\" + str(np.mean(f1_scores)) + \", std=\" + str(np.std(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import costs, gradient_descent\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Cost(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, w, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, w, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, w, x, y):\n",
    "        pass\n",
    "\n",
    "class MeanSquaredError(Cost):\n",
    "    def __call__(self, w, x, y):\n",
    "        return costs.compute_mse(y, x, w)\n",
    "\n",
    "    def predict(self, w, x):\n",
    "        return round(w.T@x)\n",
    "\n",
    "    def grad(self, w, x, y):\n",
    "        return gradient_descent.mse_gradient(y, x, w)\n",
    "\n",
    "class MeanAbsoluteError(Cost):\n",
    "    def __call__(self, w, x, y):\n",
    "        return costs.compute_mae(y, x, w)\n",
    "\n",
    "    def predict(self, w, x):\n",
    "        return round(w.T@x)\n",
    "\n",
    "    def grad(self, w, x, y):\n",
    "        return gradient_descent.mae_gradient(y, x, w)\n",
    "\n",
    "class LogisticLoss(Cost):\n",
    "    def __call__(self, w, x, y):\n",
    "        return costs.compute_log_loss(y, x, w)\n",
    "\n",
    "    def __call__(self, w, x):\n",
    "        return costs.sigmoid(w.T@x)\n",
    "\n",
    "    def grad(self, w, x, y):\n",
    "        return gradient_descent.log_gradient(y, x, w)\n",
    "\n",
    "\n",
    "class Fitter(ABC):\n",
    "    def __init__(self, cost, penalty, weight):\n",
    "        self.cost = cost\n",
    "        self.penalty = penalty\n",
    "        self.weight = weight\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.cost(x)\n",
    "\n",
    "class GradientDescent(Fitter):\n",
    "    def __init__(self, cost, penalty, max_iter):\n",
    "        super().__init__(cost, penalty)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for n_iter in range(self.max_iter):\n",
    "            self.weights = self.weights - self.gamma * self.cost.grad() \n",
    "        return self.cost(self.weights, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
