{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(\"/home/souly/Desktop/ml/ml-project1\")\n",
    "from src import json_parser, helpers, preprocessing\n",
    "from src.helpers import split_data_rand\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from src.preprocessing import undefined_to_median, undefined_to_avg, prune_undefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_csv(\"../data/raw/x_train.csv\")\n",
    "y_df = pd.read_csv(\"../data/raw/y_train.csv\")\n",
    "\n",
    "features = json_parser.parse_json_file(\"features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "seed = 42\n",
    "degree = 1\n",
    "split_ratio = 0.8\n",
    "\n",
    "x = x_df.values\n",
    "y = y_df[\"_MICHD\"].values\n",
    "\n",
    "x = preprocessing.clean_data(features, x, median_estimator=True, do_poly=False, do_one_hot=True)\n",
    "\n",
    "split_index = int(len(x)*split_ratio)\n",
    "x_train = x[:split_index]\n",
    "y_train = y[:split_index]\n",
    "x_test = x[split_index:]\n",
    "y_test = x[split_index:]\n",
    "\n",
    "positive_indices = np.where(y_train == 1)[0]\n",
    "negative_indices = np.where(y_train != 1)[0]\n",
    "\n",
    "min_samples = min(len(positive_indices), len(negative_indices))\n",
    "\n",
    "random.seed(seed)\n",
    "downsampled_negative_indices = random.sample(list(negative_indices), min_samples)\n",
    "\n",
    "balanced_indices = np.concatenate([positive_indices, downsampled_negative_indices])\n",
    "random.shuffle(balanced_indices)\n",
    "\n",
    "x_train = x_train[balanced_indices]\n",
    "y_train = y_train[balanced_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Accuracy: \" + str(model.score(x_test, y_test)))\n",
    "    pred = model.predict(x_test)\n",
    "    print(\"F1: \" + str(f1_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7779982743744608\n",
      "F1: 0.7827041634997044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/souly/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "run(LogisticRegression(penalty=\"l2\"), x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/souly/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7776531492666091\n",
      "F1: 0.7820718816067653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/souly/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "run(LogisticRegression(penalty=\"none\"), x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import costs, gradient_descent\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Cost(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, w, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, w, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, w, x, y):\n",
    "        pass\n",
    "\n",
    "class MeanSquaredError(Cost):\n",
    "    def __call__(self, w, x, y):\n",
    "        return costs.compute_mse(y, x, w)\n",
    "\n",
    "    def predict(self, w, x):\n",
    "        return round(w.T@x)\n",
    "\n",
    "    def grad(self, w, x, y):\n",
    "        return gradient_descent.mse_gradient(y, x, w)\n",
    "\n",
    "class MeanAbsoluteError(Cost):\n",
    "    def __call__(self, w, x, y):\n",
    "        return costs.compute_mae(y, x, w)\n",
    "\n",
    "    def predict(self, w, x):\n",
    "        return round(w.T@x)\n",
    "\n",
    "    def grad(self, w, x, y):\n",
    "        return gradient_descent.mae_gradient(y, x, w)\n",
    "\n",
    "class LogisticLoss(Cost):\n",
    "    def __call__(self, w, x, y):\n",
    "        return costs.compute_log_loss(y, x, w)\n",
    "\n",
    "    def __call__(self, w, x):\n",
    "        return costs.sigmoid(w.T@x)\n",
    "\n",
    "    def grad(self, w, x, y):\n",
    "        return gradient_descent.log_gradient(y, x, w)\n",
    "\n",
    "\n",
    "class Fitter(ABC):\n",
    "    def __init__(self, cost, penalty, weight):\n",
    "        self.cost = cost\n",
    "        self.penalty = penalty\n",
    "        self.weight = weight\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.cost(x)\n",
    "\n",
    "class GradientDescent(Fitter):\n",
    "    def __init__(self, cost, penalty, max_iter):\n",
    "        super().__init__(cost, penalty)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for n_iter in range(self.max_iter):\n",
    "            self.weights = self.weights - self.gamma * self.cost.grad() \n",
    "        return self.cost(self.weights, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
