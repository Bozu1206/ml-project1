import numpy as np
import fitters
import metrics
import numpy as np
x_df = pd.read_csv("../data/raw/x_train.csv")
y_df = pd.read_csv("../data/raw/y_train.csv")
x = x_df.values
y = y_df["_MICHD"].values
seed=499912
np.random.seed(seed)
def resample(x, y, seed, d_size, u_size, oversample=False, downsample=False, both=False):
    np.random.seed(seed)
    
    # Trouver les indices des classes -1 et 1
    negative_indices = np.where(y == -1)[0]
    positive_indices = np.where(y == 1)[0]
    
    if oversample:
        # Sur-échantillonnage de la classe minoritaire (1)
        num_samples = int(u_size * len(positive_indices))
        sampled_positive_indices = np.random.choice(positive_indices, num_samples, replace=True)
        resampled_x = np.concatenate([x[negative_indices], x[sampled_positive_indices]])
        resampled_y = np.concatenate([y[negative_indices], y[sampled_positive_indices]])
        
    elif downsample:
        # Sous-échantillonnage de la classe majoritaire (-1)
        num_samples = int(d_size * len(negative_indices))
        sampled_negative_indices = np.random.choice(negative_indices, num_samples, replace=False)
        resampled_x = np.concatenate([x[sampled_negative_indices], x[positive_indices]])
        resampled_y = np.concatenate([y[sampled_negative_indices], y[positive_indices]])
        
    elif both:
        # Sous-échantillonnage de la classe majoritaire (-1) et sur-échantillonnage de la classe minoritaire (1)
        num_negative_samples = int(d_size * len(negative_indices))
        num_positive_samples = int(u_size * len(positive_indices))
        sampled_negative_indices = np.random.choice(negative_indices, num_negative_samples, replace=False)
        sampled_positive_indices = np.random.choice(positive_indices, num_positive_samples, replace=True)
        resampled_x = np.concatenate([x[sampled_negative_indices], x[sampled_positive_indices]])
        resampled_y = np.concatenate([y[sampled_negative_indices], y[sampled_positive_indices]])
        
    else:
        raise ValueError("Vous devez définir au moins un des paramètres : oversample, downsample, ou both.")
        
    return resampled_x, resampled_y


# Appliquer la fonction de rééchantillonnage
y_train, x_train, y_test, x_test = helpers.split_data_rand(y, x, 0.7)
print(y_train.shape, x_train.shape)

x_train, y_train = resample(x_train, y_train, seed=seed, d_size=0.18, u_size=1, oversample=False, downsample=False, both=True)

# Vérifier les résultats
print("Nombre d'échantillons dans la classe -1:", sum(y_train == -1))
print("Nombre d'échantillons dans la classe 1:", sum(y_train == 1))

features = parse_json_file("features.json")
x_train = clean_data(features, x_train, do_poly=False)
x_test = clean_data(features, x_test, do_poly=False)

print("# of samples:", x_train.shape[0])
print("# of features:", x_train.shape[1])

# Logistic Regression
print(style.BOLD + f"\nLogistic Regression GD for dataset D_{2 + 1}" + style.RESET)
print("========================================================================".replace("=", "-"))
rrf = fitters.RidgeRegressionFitter(y_train, x_train, y_test, x_test, 10e-7, 0.15)
w, loss = rrf.fit()
_print_result(rrf, y_test, x_test, w)
x_true = pd.read_csv("../data/raw/x_test.csv")
ids = x_true.Id
x_true = clean_data(features, x_true.values, do_poly=False)
test_preds = rrf.predict(x_true, w)
helpers.create_csv_submission(ids, test_preds, f"testRRF-mine.csv")
print("========================================================================".replace("=", "-"))
